{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 4 quick start\n",
    "\n",
    "***\n",
    "\n",
    "## Basic usage\n",
    "\n",
    "This notebook shows a basic usage of the `deepracer_gym` package along with some utility functions in `src.utils`.\n",
    "\n",
    "Make sure that you have completed the setup from the `SETUP.md` file and are using the proper python environment with this notebook.\n",
    "\n",
    "### Start the simulation service\n",
    "\n",
    "Start the simulation service container with the following command. Using this for the very first time may take some time (upto ~10 or 15 minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source scripts/restart_deepracer.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may also find other scripts under `scripts/` similarly useful to stop or restart the simulation service, etc.\n",
    "\n",
    "To check if the container is rumming you can use the following commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "apptainer instance list ||  # if using Apptainer (PACE ICE)\n",
    "docker ps                   # if using Docker (local setup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the simulator is initialized by the config files in the `configs/` directory. To change the simulation settings, restart it after changing the files in `configs/` accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interact with simulation via `deepracer_gym`\n",
    "\n",
    "We can interact with the simulation service using the familiar `gymnasium` API via the `deepracer-v0` environment provided by the `deepracer_gym` package (under `packages/`).\n",
    "\n",
    "Simply import `deepracer_gym` before using the `deepracer-v0` environment with `gymnasium` as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import deepracer_gym\n",
    "\n",
    "env = gym.make(\n",
    "    'deepracer-v0'\n",
    ")\n",
    "\n",
    "observation, info = env.reset()\n",
    "\n",
    "observation, reward, terminated, truncated, info = env.step(\n",
    "    env.action_space.sample()\n",
    ")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the output dimensions of the observations\n",
    "{\n",
    "    k: v.shape for k, v in observation.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Utility functions and features\n",
    "\n",
    "### Flattened environment\n",
    "\n",
    "Notice that the `observation` variable above is a dictionary (keys are sensor names, values are measurements). Such a data-structure is a bit more difficult to handle than simple vectors, especially for batching purposes. Therefore, we suggest that you use the provided `utils.make_environmrnt` function instead. It 'flattens' the observation space into a single vector space using the `gymnasium.wrappers.FlattenObservation` class.\n",
    "\n",
    "Additionally, it also wraps the environment with a `gymnasium.wrappers.RecordEpisodeStatistics` class, which can be very convenient for calculating things like episode langth and returns.\n",
    "\n",
    "Please get familiar with all of these functions/ classes before attempting the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import make_environment\n",
    "\n",
    "env = make_environment(         # just replace gym.make\n",
    "    'deepracer-v0'\n",
    ")\n",
    "\n",
    "observation, info = env.reset()\n",
    "\n",
    "observation, reward, terminated, truncated, info = env.step(\n",
    "    env.action_space.sample()\n",
    ")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the output dimensions of the observations\n",
    "observation.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize agent policy\n",
    "\n",
    "So long as your agent implements a `get_action` method as in `src.agents.py`, you can use our provided `src.utils.demo` function to visualize the policy of the agent in the form of a MP4 video saved under `demos/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import demo\n",
    "from src.agents import RandomAgent\n",
    "\n",
    "agent = RandomAgent(environment=env)\n",
    "demo(agent.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on multiple tracks\n",
    "\n",
    "You can use the provided `src.utils.evaluate` function to evaluate your agent on all three project tracks (for any specified race type in `configs/environment_configs.yaml`). The results are both returned as well as saved (and over-written) under `evaluations/`.\n",
    "\n",
    "Please note however that this funciton basically re-starts the simulation a number of times to switch between the tracks for evaluation and this may be a bit time-consuming (~5 minutes on a PACE ICE machine for an untrained agent, and ~20 minutes for a fully trained agent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import evaluate\n",
    "\n",
    "metrics = evaluate(\n",
    "    agent.eval()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting evaluation metrics\n",
    "\n",
    "You can plot the returned evaluation metrics dictionary using the provided `src.utils.plot_metrics` functions. The results are also saved under `plots/`.\n",
    "\n",
    "Note however that you do not necessarily have to stick to this exact visualization/plot, and may make adjustments as you see fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import plot_metrics\n",
    "\n",
    "plot_metrics(\n",
    "    metrics, title='usage'\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Training and logging\n",
    "\n",
    "Please use the structure in `src.run.py` to design your training and logging loops. Importantly, try to use `tensorboard` if you can, for example as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.run import run\n",
    "\n",
    "my_hyper_parameters = {\n",
    "    'lr': 4e-4\n",
    "}\n",
    "run(my_hyper_parameters)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view the training logs, use `tensorboard` by running the following command in your terminal.\n",
    "\n",
    "```bash\n",
    "tensorboard --logdir runs\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepracer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
